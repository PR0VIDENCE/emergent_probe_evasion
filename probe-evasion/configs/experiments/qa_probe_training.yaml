# QA-based probe training pipeline configuration
# Trains probes on activations from actual QwQ-32B generations
# to close the distribution gap between short statements and long CoT outputs.
# Answers are constrained to 1-3 sentences to keep concept signal concentrated;
# the reasoning trace (inside <think>...</think>) is unconstrained.

concept: trees
model_config: configs/models/qwq_32b.yaml
data_dir: data/concepts/trees_qa
target_layers: [8, 20, 32, 44, 56]

token_positions:
  - last_token
  - end_of_reasoning
  - first_answer_sentence_end
  - answer_mean_pool

generation:
  max_new_tokens: 2048
  temperature: 0.6
  top_p: 0.95
  top_k: 20

probe_training:
  random_seeds: [42, 123, 456, 789]
  learning_rate: 0.001
  num_epochs: 100
  weight_decay: 0.1
  patience: 10
  normalize: true

split:
  train: 0.8
  val: 0.1
  test: 0.1

supplementary:
  handwritten_validation: data/concepts/trees_qa/handwritten_validation.yaml
  adversarial_test_set: data/concepts/trees_qa/adversarial_test_set.yaml

storage:
  base_dir: /workspace/probe_data
