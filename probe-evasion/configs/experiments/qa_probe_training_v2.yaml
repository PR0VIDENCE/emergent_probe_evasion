# v2 QA-based probe training pipeline configuration
# 975 contrastive pairs (800 base + 175 prefix variants)
# across 5 groups: A (handwritten), B (diverse LLM), C (botanical near-miss),
# D (vocabulary/semantic near-miss), E (indirect tree refs), A_prefix (style variants)
#
# Key changes from v1:
# - 15 target layers (every 4th) instead of all 64
# - Pre-assigned splits for reproducibility
# - Top-5 layer selection + logistic regression combiner
# - answer_mean_pool as primary position

concept: trees
model_config: configs/models/qwq_32b.yaml
data_dir: data/concepts/trees_qa_v2
target_layers: [4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60]

token_positions:
  - last_token
  - end_of_reasoning
  - first_answer_sentence_end
  - answer_mean_pool

generation:
  max_new_tokens: 2048
  temperature: 0.6
  top_p: 0.95
  top_k: 20

probe_training:
  random_seeds: [42, 123, 456, 789]
  learning_rate: 0.001
  num_epochs: 100
  weight_decay: 0.1
  patience: 10
  normalize: true
  top_k_layers: 5
  primary_position: answer_mean_pool

split:
  method: preassigned
  split_file: data/concepts/trees_qa_v2/split_assignment.yaml
  train: 0.8
  val: 0.1
  test: 0.1

combiner:
  enabled: true
  regularization_C: 1.0
  output_path: configs/probes/weighted_combination_v2.yaml

storage:
  base_dir: /workspace/probe_data_v2
