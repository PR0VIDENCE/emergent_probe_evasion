# Probe Evasion in Reasoning Models

Early-stage research investigating whether language models can learn to evade linear probes in-context when informed about the probing setup.

## Research Question

Can reasoning models learn to suppress or modify internal representations when they are informed that their activations are being monitored by linear probes for specific concepts?

## Experimental Conditions

1. **Baseline** â€” Model answers questions normally without any information about monitoring
2. **Informed** â€” Model is told how linear probes work, what concept is being probed, and that its reasoning trace will be monitored
3. **Informed-No-Monitor** â€” Same information as Informed, but explicitly states the reasoning trace will NOT be monitored

## Project Structure

- `configs/` â€” YAML configuration files for models, probes, and experiments
- `src/` â€” Core source code for probes, prompts, inference, and analysis
- `experiments/` â€” Individual experiment configurations and run scripts
- `data/` â€” Storage for probes, activations, and outputs
- `scripts/` â€” Utility scripts for cluster setup and parameter sweeps

## Status

ðŸš§ Early-stage research â€” scaffolding only, no implemented logic yet.
